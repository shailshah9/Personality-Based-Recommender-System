{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the input data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# XTrain = np.loadtxt('training_data.csv', skiprows=1, usecols=(0,1), delimiter=',', dtype='str')\n",
    "# print XTrain[0]\n",
    "# # XTest = np.loadtxt('test_data_public_new.csv', skiprows=1, usecols=(1,), delimiter=',',dtype='str')\n",
    "# # print XTest[0]\n",
    "# YTrain = XTrain[:,-1]\n",
    "# XTrain = XTrain[:,0]\n",
    "# print XTrain.shape\n",
    "# YTrain = YTrain.astype(np.int)\n",
    "# # YTrain = YTrain.reshape(-1,1)\n",
    "# print type(YTrain[0]),YTrain[0],YTrain.shape\n",
    "# # print XTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "#Read the tweets one by one and process it\n",
    "import csv\n",
    "# inpTweets = csv.reader(open('TwitterData/survey_dump.csv', 'rb'), delimiter=',')\n",
    "inpTweets = csv.reader(open('TwitterData/survey_dump_with_tweet_count', 'rt'), delimiter=',')\n",
    "i = 0\n",
    "for row in inpTweets:\n",
    "    i+=1;\n",
    "\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class PreprocessTweets:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = 'PreprocessTweets'\n",
    "\n",
    "    #start process_tweet\n",
    "    def processTweet(self, tweet):\n",
    "        \n",
    "        #Convert to lower case\n",
    "        tweet = tweet.lower()\n",
    "        #Convert www.* or https?://* to URL\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "        #Convert @username to AT_USER\n",
    "        tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "        #Remove additional white spaces\n",
    "        tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "        #Remove special characters\n",
    "        #tweet = re.sub('*\\[\\]%\\(\\)', '', tweet)\n",
    "        #Replace #word with word\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "        #trim\n",
    "        tweet = tweet.strip('\\'\"')\n",
    "\n",
    "        # Remove all Non-ASCII characters\n",
    "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "        return tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import PreprocessTweets\n",
    "\n",
    "class FilterStopWords:\n",
    "\n",
    "    # stopWords = []\n",
    "    def __init__(self):\n",
    "        self.name = 'FilterStopWords'\n",
    "        #initialize stopWords\n",
    "        self.stopWords = []\n",
    "\n",
    "    #start replaceTwoOrMore\n",
    "    # def replaceTwoOrMore(s):\n",
    "    #     #look for 2 or more repetitions of character and replace with the character itself\n",
    "    #     pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    #     return pattern.sub(r\"\\1\\1\", s)\n",
    "    #end\n",
    "\n",
    "    def getStopWordList(self, stopWordListFileName):\n",
    "        #read the stopwords file and build a list\n",
    "        stopWords = []\n",
    "        stopWords.append('AT_USER')\n",
    "        stopWords.append('URL')\n",
    "        stopWords.append('[')\n",
    "        stopWords.append('[')\n",
    "\n",
    "        fp = open(stopWordListFileName, 'r')\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            word = line.strip()\n",
    "            stopWords.append(word)\n",
    "            line = fp.readline()\n",
    "        fp.close()\n",
    "        return stopWords\n",
    "    \n",
    "    def getFeatureVector(self, tweet, stopWords):\n",
    "        featureVector = []\n",
    "        #split tweet into words\n",
    "        words = tweet.split()\n",
    "        for w in words:\n",
    "            #replace two or more with two occurrences\n",
    "            #w = replaceTwoOrMore(w)\n",
    "            #strip punctuation\n",
    "            w = w.strip('\\'\"?,.')\n",
    "            #check if the word stats with an alphabet\n",
    "            val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "            #ignore if it is a stop word\n",
    "            if(w in self.stopWords or val is None):\n",
    "                continue\n",
    "            else:\n",
    "                featureVector.append(w.lower())\n",
    "        return featureVector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XTrain = []\n",
    "# YTrain = []\n",
    "# XTrainFeatures = []\n",
    "# XTrainSentiment = []\n",
    "# XTrainFreqTweets = []\n",
    "# geo_latitude = []\n",
    "# geo_longitude = []\n",
    "\n",
    "# # sample = \"{\"\"“@AMBITIOUS_SLIM: Fresh could have deleted my one of my double haves” huh\"\",\"\"@DJFreshery cause I can see u snappin off but doing it lowkey\"\",\"\"“@RAWmartini: The #WellCumThruThenMovement” best movement ever\"\",\"\"@HennessyBronze I do what I want tho\"\"}\"\n",
    "# # sample = sample.replace('\"\",\"\"',\" \")\n",
    "# # sample = sample.replace('\"\"',\" \")\n",
    "# # print sample\n",
    "# # wordsList = sample.split()\n",
    "\n",
    "\n",
    "# # newwordsList = [word.split() for word in wordsList]\n",
    "# # print newwordsList\n",
    "# # filtered_words = [word for word in newwordsList if word not in stopwords.words('english')]\n",
    "# # print filtered_words[0]\n",
    "# # filteredTweets = ' '.join(filtered_words)\n",
    "# # print filteredTweets\n",
    "\n",
    "\n",
    "# # from PreprocessTweets import PreprocessTweets\n",
    "# # from FilterStopWords import FilterStopWords\n",
    "# import nltk\n",
    "# # from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# # nltk.download()\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# class FeatureEngineering:\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.name = 'FeatureEngineering'\n",
    "#         self.featureList = []\n",
    "#         # self.sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "#     #start extract_features\n",
    "#     def extract_features(self,tweet):\n",
    "#         tweet_words = set(tweet)\n",
    "#         features = {}\n",
    "#         for word in self.featureList:\n",
    "#             features['contains(%s)' % word] = (word in tweet_words)\n",
    "#         return features\n",
    "\n",
    "# ## Create New Training set based on personality labels predicted from Survey results\n",
    "\n",
    "#     def createNewTrainingSet(self):\n",
    "\n",
    "#         objFilterStopWords = FilterStopWords()\n",
    "#         objPreprocessTweets = PreprocessTweets()\n",
    "\n",
    "#         stopWords = objFilterStopWords.getStopWordList('TwitterData/StopWords.txt')\n",
    "        \n",
    "        \n",
    "#         #Read the tweets one by one and process it\n",
    "# #         inpTweets = csv.reader(open('TwitterData/survey_dump.csv', 'rb'), delimiter=',') #, quotechar='|')\n",
    "#         inpTweets = csv.reader(open('TwitterData/survey_dump_with_tweet_count', 'rb'), delimiter=',')\n",
    "#         inpTweets.next()\n",
    "#         tweets = []\n",
    "#         i = 0\n",
    "#         for row in inpTweets:\n",
    "# #             print row\n",
    "#             personality = row[5]\n",
    "# #             print personality\n",
    "#             tweet = row[1]\n",
    "#             cleanTweet = tweet.replace('\"\",\"\"',\" \")\n",
    "#             cleanTweet = cleanTweet.replace('\"\"',\" \")\n",
    "# #             print tweet\n",
    "#             processedTweet = objPreprocessTweets.processTweet(cleanTweet)\n",
    "# #             print processedTweet\n",
    "\n",
    "#             XTrainFreqTweets.append(int(row[4]))\n",
    "#             wordsList = processedTweet.split()\n",
    "# #             print wordsList\n",
    "            \n",
    "#             # Remove stop words\n",
    "# #             filtered_words = [word for word in processedTweet if word not in stopwords.words('english')]\n",
    "#             filtered_words = [word for word in wordsList if word not in stopwords.words('english')]\n",
    "# #             print filtered_words\n",
    "#             filteredTweets = ' '.join(filtered_words)\n",
    "            \n",
    "#             featureVector = objFilterStopWords.getFeatureVector(processedTweet, stopWords)\n",
    "            \n",
    "#             geo_latitude.append(float(row[2]))\n",
    "#             geo_longitude.append(float(row[3]))\n",
    "            \n",
    "#             # Append to feature list to collect total words\n",
    "# #             for word in featureVector:\n",
    "# #                 self.featureList.append(word)\n",
    "#             # featureList.append([featureVector[i] for i in xrange(len(featureVector))])\n",
    "\n",
    "#             # Use NLTK Vader for Sentiment Analysis\n",
    "\n",
    "#             # Citation: Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text.\n",
    "#             # Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "#             # Extract sentiment based on the tweet.\n",
    "#             # ss = self.sid.polarity_scores(row)\n",
    "#             # for k in sorted(ss):\n",
    "#             #     print('{0}: {1}, '.format(k, ss[k]))\n",
    "#             #\n",
    "#             # totSentiment = sorted(ss)[0]\n",
    "\n",
    "#             # Use TextBlog for Sentiment Analysis\n",
    "#             # print tweet\n",
    "#             # blob = TextBlob(tweet)\n",
    "            \n",
    "#             blob = TextBlob(processedTweet)\n",
    "#             # print blob\n",
    "#             sentiment = 0\n",
    "#             for sentence in blob.sentences:\n",
    "#                 # print sentence\n",
    "#                 sentiment += sentence.sentiment.polarity\n",
    "#                 # print sentiment\n",
    "\n",
    "#             totSentiment = sentiment/ len(blob.sentences)\n",
    "# #             featureVector.append(totSentiment)\n",
    "\n",
    "#             XTrainSentiment.append(totSentiment)\n",
    "    \n",
    "# #             strFeatures = [item.lower() for item in featureVector]\n",
    "            \n",
    "# #             XTrainFeatures.append(processedTweet)\n",
    "#             XTrainFeatures.append(filteredTweets)\n",
    "            \n",
    "#             YTrain.append(personality)\n",
    "            \n",
    "#             tweets.append((featureVector, personality))\n",
    "            \n",
    "# #             i+=1\n",
    "# #             if i==3:\n",
    "# #                 break\n",
    "            \n",
    "#         #end loop\n",
    "# #         print tweets\n",
    "# #         print self.featureList\n",
    "#         # Remove featureList duplicates\n",
    "# #         featureList = list(set(self.featureList))\n",
    "\n",
    "#         # Extract feature vector for all tweets in one shote\n",
    "#         training_set = nltk.classify.util.apply_features(self.extract_features, tweets)\n",
    "\n",
    "# #         print self.featureList\n",
    "# #         print training_set\n",
    "\n",
    "\n",
    "       \n",
    "#         return training_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class FeatureEngineering:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = 'FeatureEngineering'\n",
    "        self.featureList = []\n",
    "        # self.sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    #start extract_features\n",
    "    def extract_features(self,tweet):\n",
    "        tweet_words = set(tweet)\n",
    "        features = {}\n",
    "        for word in self.featureList:\n",
    "            features['contains(%s)' % word] = (word in tweet_words)\n",
    "        return features\n",
    "\n",
    "## Create New Training set based on personality labels predicted from Survey results\n",
    "\n",
    "    def createNewTrainingSet(self, fileName):\n",
    "        XTrain = []\n",
    "        YTrain = []\n",
    "        XTrainFeatures = []\n",
    "        XTrainSentiment = []\n",
    "        XTrainFreqTweets = []\n",
    "        geo_latitude = []\n",
    "        geo_longitude = []\n",
    "        \n",
    "        objFilterStopWords = FilterStopWords()\n",
    "        objPreprocessTweets = PreprocessTweets()\n",
    "\n",
    "        stopWords = objFilterStopWords.getStopWordList('TwitterData/StopWords.txt')\n",
    "        \n",
    "        #Read the tweets one by one and process it\n",
    "        inpTweets = csv.reader(open(fileName, 'r'), delimiter=',')\n",
    "        next(inpTweets)\n",
    "        tweets = []\n",
    "        i = 0\n",
    "        for row in inpTweets:\n",
    "#             print row\n",
    "            personality = row[5]\n",
    "            tweet = row[1]\n",
    "            cleanTweet = tweet.replace('\"\",\"\"',\" \")\n",
    "            cleanTweet = cleanTweet.replace('\"\"',\" \")\n",
    "            processedTweet = objPreprocessTweets.processTweet(cleanTweet)\n",
    "\n",
    "            XTrainFreqTweets.append(int(row[4]))\n",
    "            wordsList = processedTweet.split()\n",
    "            \n",
    "            # Remove stop words\n",
    "            filtered_words = [word for word in wordsList if word not in stopwords.words('english')]\n",
    "            filteredTweets = ' '.join(filtered_words)\n",
    "            \n",
    "            featureVector = objFilterStopWords.getFeatureVector(processedTweet, stopWords)\n",
    "            \n",
    "            geo_latitude.append(float(row[2]))\n",
    "            geo_longitude.append(float(row[3]))\n",
    "            \n",
    "            blob = TextBlob(processedTweet)\n",
    "            sentiment = 0\n",
    "            for sentence in blob.sentences:\n",
    "                sentiment += sentence.sentiment.polarity\n",
    "\n",
    "            totSentiment = sentiment/ len(blob.sentences)\n",
    "\n",
    "            XTrainSentiment.append(totSentiment)\n",
    "\n",
    "            XTrainFeatures.append(filteredTweets)\n",
    "            \n",
    "            YTrain.append(personality)\n",
    "                        \n",
    "#             i+=1\n",
    "#             if i==3:\n",
    "#                 break\n",
    "            \n",
    "\n",
    "        return XTrain, YTrain, XTrainFeatures, XTrainSentiment, XTrainFreqTweets, geo_latitude, geo_longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objFeatureEngineering = FeatureEngineering()\n",
    "fileName = 'TwitterData/survey_dump_with_tweet_count'\n",
    "XTrain, YTrain, XTrainFeatures, XTrainSentiment, XTrainFreqTweets, geo_latitude, geo_longitude = objFeatureEngineering.createNewTrainingSet(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName = 'TwitterData/survey_dump_geo_gt_8_1'\n",
    "XEval, YEval, XEvalFeatures, XEvalSentiment, XEvalFreqTweets, eval_geo_latitude, eval_geo_longitude = objFeatureEngineering.createNewTrainingSet(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get Feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # from PreprocessTweets import PreprocessTweets\n",
    "# # from FilterStopWords import FilterStopWords\n",
    "# # from FeatureEngineering import FeatureEngineering\n",
    "# import nltk\n",
    "\n",
    "\n",
    "# objFilterStopWords = FilterStopWords()\n",
    "# objPreprocessTweets = PreprocessTweets()\n",
    "# objFeatureEngineering = FeatureEngineering()\n",
    "\n",
    "# #trainingSet = objFeatureEngineering.createTrainingSet()\n",
    "# trainingSet = objFeatureEngineering.createNewTrainingSet()\n",
    "\n",
    "# stopWordListFileName = 'TwitterData/StopWords.txt'\n",
    "# stopWords = objFilterStopWords.getStopWordList(stopWordListFileName)\n",
    "\n",
    "# # Train the classifier\n",
    "# NBClassifier = nltk.NaiveBayesClassifier.train(trainingSet)\n",
    "\n",
    "# # Test the classifier\n",
    "# testTweet = 'Hurray, I am working on a project on personality prediction on twitter data using sentiment analysis!'\n",
    "# processedTestTweet = objPreprocessTweets.processTweet(testTweet)\n",
    "# featureVector = objFilterStopWords.getFeatureVector(processedTestTweet, stopWords)\n",
    "# # print NBClassifier.classify(objFeatureEngineering.extract_features(featureVector))\n",
    "\n",
    "\n",
    "# # # print informative features about the classifier\n",
    "# # print NBClassifier.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "# # testTweet = 'I have successfully completed this project.'\n",
    "# # processedTestTweet = objPreprocessTweets.processTweet(testTweet)\n",
    "# # featureVector = objFilterStopWords.getFeatureVector(processedTestTweet, stopWords)\n",
    "# # print NBClassifier.classify(objFeatureEngineering.extract_features(featureVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newYTrain = []\n",
    "# print YTrain\n",
    "for item in YTrain:\n",
    "    temp = item.replace('[', '')\n",
    "    temp = temp.replace('\\\"', '')\n",
    "    newItem = temp.replace(']', '')\n",
    "    newYTrain.append(newItem)\n",
    "    \n",
    "YTrain = newYTrain\n",
    "# print YTrain\n",
    "# print XTrainFeatures[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the class labels to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def mapLabels(className):\n",
    "    if className == 'Conscientiousness':\n",
    "        return 0\n",
    "    elif className == 'Extrovert':\n",
    "        return 1\n",
    "    elif className == 'Agreeable':\n",
    "        return 2\n",
    "    elif className == 'Empathetic':\n",
    "        return 3\n",
    "    elif className == 'Novelty Seeking':\n",
    "        return 4\n",
    "    elif className == 'Perfectionist':\n",
    "        return 5\n",
    "    elif className == 'Rigid':\n",
    "        return 6\n",
    "    elif className == 'Impulsive':\n",
    "        return 7\n",
    "    elif className == 'Psychopath':\n",
    "        return 8\n",
    "    elif className == 'Obsessive':\n",
    "        return 9\n",
    "#     elif className == None:\n",
    "#         return 10\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "YTrain = [mapLabels(x) for x in YTrain]\n",
    "YEval = [mapLabels(x) for x in YEval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print (YEval[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "{\"2 please\",\" AT_USER 1dmoviepremiere today! :d :d\",\"AT_USER getting follow would blessing :) love moon back <3 j\",\"AT_USER getting follow would blessing :) love moon back <3 k\",\"AT_USER thank bby :))\",\"AT_USER getting follow would blessing :) love moon back <3 37\",\"AT_USER getting follow would blessing :) love moon back <3 40\",\"AT_USER getting follow would blessing :) love moon back <3 30\",\"AT_USER idk little awkward tbh\",\"AT_USER *google images* big booty hoes\",\"AT_USER afternoon good sir\",\"AT_USER think ketchup enjoy it\",\"AT_USER heart aw aw aw\",\"AT_USER ugh heard yet :(\",\"AT_USER getting follow would blessing :) love moon back <3 69\",\"AT_USER can't wait till come back tour us :))\",\" please follow fuckers love much AT_USER AT_USER AT_USER AT_USER 35\",\"calum hood stop dick follow please\",\"uk x factor different compared us version like much better lol\",\"AT_USER he's jealous\",\"AT_USER cause anxiety\",\"AT_USER angel sahar lol ily <3 liking new @ btw\",\"AT_USER hey calum see follow dont ignore please ily 127\",\"AT_USER dogs cats?\",\"AT_USER youtuber....\",\"AT_USER one us\",\"lol icon me\",\"AT_USER hi how's recording?\",\"AT_USER hi calum! hope wonderful day staying healthy :) please follow me, love know 8\",\"idk still \\\"what's goodie\\ ,\"AT_USER hey fuck you\",\" AT_USER please follow me! love much <33 t\",\"AT_USER lol caused enough drama flirting shit us\",\"AT_USER actually think bendy before?\",\" AT_USER we're late guys lol suck hold would've though\",\"this follow party getting hand\",\"AT_USER late try get follows, wait till tomorrow afternoon man\",\"AT_USER holy crap, gonna peel much\",\"AT_USER join marijuana movement, joint effort ha that's funny follow pls ily \",\"AT_USER bet could totally dude!! give twitter name stuff hopefully they'll follow you!\",\"he's holding balls\",\"AT_USER skype part hahaha i'm crying\",\" AT_USER AT_USER AT_USER AT_USER follow motherfuckers ily i'm 8\",\"ill keep eyes wide opennn\",\" AT_USER pineapple cool refreshing drink inside. turn up\",\"never forget calum's lip piercing\",\"AT_USER hey calum see follow dont dick please ily 307\",\"being added list one best feelings ( ) ily\",\"AT_USER mean better first one claps \",\"AT_USER meow bitch\",\"AT_USER hey calum see follow dont dick 218\",\"AT_USER hey calum see follow dont dick please 379\",\"i can't even watch damn hug without wanting stab throat\",\"AT_USER what's favorite thing bus?\",\"AT_USER god bless america\",\"AT_USER man things\",\"the live tweets boys friends best ones\",\"AT_USER can't hahaa\",\"AT_USER love much even know. thank much everything <3 goodnight boo\",\"i wonder shirt ashton ripped...\",\"AT_USER ...they already did....\",\"i'm uncomfortable luke creeps see holla me? maybe follow me? idk choice\",\"AT_USER wow gonna confused lol\",\"AT_USER AT_USER gonna buy strippers?\",\" AT_USER hi bby! please follow me! love much <33 138\",\" AT_USER hi bby! please follow me! love much <33 106\"}\n",
      "8\n",
      "{\"i wanna see cousin today dont feel like going mondawmin man \",\" AT_USER yo wanted smack shit freshman today yo. !!!!!!\",\" AT_USER vma's sunday!!! \",\"ya bitches asking asking nigga do\",\" AT_USER ain't get locked summer im proud \",\"AT_USER lol could've said thanks & tell ya name then.\",\" AT_USER joy luck club boringgggg! !!!\",\"AT_USER omg, going too?!\",\"ima things right time ya, born ride \",\"AT_USER AT_USER lmfao go 'head & get fcked shit.\",\"i wanna go kona grill.\",\"AT_USER sorry ya lost, keep ya head up.\",\"i cant stop smiling \",\"yo really know every vine \",\"pussy, money, weed got lat...steal ya girl call hijack\",\" AT_USER know wear tomorrow . goin? lol\",\"damn. ima bitch \",\"AT_USER get hair done got that\",\" AT_USER smiley fucked fucked way \",\"single & nobody attention.\",\"morning store walk..lol\",\" AT_USER could really go pizza & wings right now! !!!!\",\" AT_USER AT_USER AT_USER got lied shit crazy \",\"bad azz nigga. coming home soon nigga.\",\"yall using videos sluts.\",\"i videos doe \",\"i guess means nie go sleep lol\",\"man, miss dad everyday.\",\"worldwide coast coast love get.\",\" AT_USER AT_USER lol fine fine. lol\",\"AT_USER need ya help \",\"i really looking forward going mall\",\"AT_USER dont that? lol\",\"the closer get \",\"lol done got dropped steps,tvs fell head everything else thats act act.\",\"im like meek never sleep\",\"i feel terrible.\",\" AT_USER dfl nie flashy avi lmao already man.\",\"that's lor corey running niggas try fight \",\"va life gonna good \",\" AT_USER can't wait go home \",\"AT_USER lee coming tonight lol\",\" AT_USER AT_USER lol rude eat 20 piece mcdouble hungry ass lmao thats somebody hungry\",\"AT_USER really dont lol right, whatever \",\"AT_USER what?\",\"so ajayasia like nah? lol\",\"AT_USER picture lmao\",\"AT_USER lol smart?\",\" AT_USER bitchesssssssssss get rings next school year yaaasss lmfao remember said that?!\",\"when finally find somebody >\",\" AT_USER yup white-tee \",\"AT_USER text eddie \",\"my phone dry \",\"AT_USER goodmorning \",\" AT_USER AT_USER nah hate guess thats cool \",\"i bust nut im back thinking money\",\"he really said girls trynna freak says, \\\"wanna ride face use eats handle bars?\\\" like ?!\",\" AT_USER never apologize feel. that's like saying sorry real \",\"i even sleep long felt like forever \",\"i wanna love .. \",\" AT_USER really hate people . \"}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "XTrain = np.array(XTrainFeatures)\n",
    "YTrain = np.array(YTrain)\n",
    "\n",
    "print (len(XTrain))\n",
    "print (len(YTrain))\n",
    "\n",
    "print (XTrain[1])\n",
    "print (YTrain[1])\n",
    "\n",
    "print (XTrain[15])\n",
    "print (YTrain[15])\n",
    "\n",
    "XEval = np.array(XEvalFeatures)\n",
    "YEval = np.array(YEval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "19\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "trainSamples = XTrain[0:60]\n",
    "YtrainSamples = YTrain[0:60]\n",
    "\n",
    "testSamples = XTrain[60:]\n",
    "YtestSamples = YTrain[60:]\n",
    "\n",
    "print (len(trainSamples))\n",
    "print (len(testSamples))\n",
    "\n",
    "# print XTrain[60:63]\n",
    "print (len(XTrain))\n",
    "\n",
    "\n",
    "trainSentimentSamples = np.array(XTrainSentiment[0:60])\n",
    "testSentimentSamples = np.array(XTrainSentiment[60:])\n",
    "trainFreqTweetSamples = np.array(XTrainFreqTweets[0:60])\n",
    "testFreqTweetSamples = np.array(XTrainFreqTweets[60:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4909\n",
      "(60, 4909)\n",
      "(19, 4909)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "XTr = vectorizer.fit_transform(trainSamples)\n",
    "print (len(vectorizer.get_feature_names()))\n",
    "trainBagVector = XTr.toarray()\n",
    "print (trainBagVector.shape)\n",
    "XTe = vectorizer.transform(testSamples)\n",
    "testBagVector = XTe.toarray()\n",
    "print (testBagVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37012\n",
      "(3995, 37012)\n"
     ]
    }
   ],
   "source": [
    "XEv = vectorizer.fit_transform(XEval)\n",
    "print (len(vectorizer.get_feature_names()))\n",
    "evalBagVector = XEv.toarray()\n",
    "print (evalBagVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3995, 4914)\n"
     ]
    }
   ],
   "source": [
    "evalBagVector = evalBagVector[:,0:4914]\n",
    "print (evalBagVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA as sklearnPCA\n",
    "# sklearn_pca = sklearnPCA(n_components=4914)\n",
    "# evalBagVectorPCA = sklearn_pca.fit_transform(evalBagVector.T)\n",
    "# print evalBagVectorPCA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainBagVector = trainSamples\n",
    "# testBagVector = testSamples\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# transformer = TfidfTransformer()\n",
    "# # print transformer   \n",
    "# tfidfTrain = transformer.fit_transform(trainBagVector)\n",
    "# tfidfTrain = tfidfTrain.toarray()\n",
    "# tfidfTest = transformer.fit_transform(testBagVector)\n",
    "# tfidfTest = tfidfTest.toarray()\n",
    "# print tfidfTrain.shape, tfidfTest.shape\n",
    "# print tfidfTrain[0]\n",
    "# print tfidfTest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f=open(\"trainBagVector.txt\",'w')\n",
    "# f.write(trainBagVector)\n",
    "# np.savetxt(\"trainBagVector.txt\",trainBagVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37012 37012\n"
     ]
    }
   ],
   "source": [
    "stateDict = {}\n",
    "featureVectors = vectorizer.get_feature_names()\n",
    "for i in range(len(featureVectors)):\n",
    "    stateDict[featureVectors[i]] = i+1\n",
    "print (len(stateDict), len(featureVectors)) #, stateDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createStateTransitionVector(categoricalState, stateDict, maxLength):\n",
    "    if categoricalState:\n",
    "        feature = []\n",
    "        for state in categoricalState.split(' '):\n",
    "            try:\n",
    "                feature.append(stateDict[state.lower()])\n",
    "            except KeyError:\n",
    "                pass\n",
    "#                 print state\n",
    "        if len(feature) != maxLength:\n",
    "            for i in range(maxLength-len(feature)):\n",
    "                feature.append(0)\n",
    "        assert(len(feature)==maxLength)\n",
    "        return feature\n",
    "    else:\n",
    "        return [0] * maxLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createStateVectors(XStates, stateDict, maxLength):\n",
    "    XFeatures = []\n",
    "    for state in XStates:\n",
    "        XFeatures.append(createStateTransitionVector(state, stateDict, maxLength))\n",
    "    return XFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainStateTransitionVector =  createStateVectors(trainSamples, stateDict,9353)\n",
    "testStateTransitionVector = createStateVectors(testSamples, stateDict,9353)\n",
    "# print trainStateTransitionVector[:2], testStateTransitionVector[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9353\n",
      "9353\n"
     ]
    }
   ],
   "source": [
    "print (max([len(i) for i in trainStateTransitionVector]))\n",
    "print (max([len(i) for i in testStateTransitionVector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Grams as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "43998\n",
      "(60, 43998)\n",
      "(19, 43998)\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "noNGram = 3\n",
    "vectorizerNGram = CountVectorizer(ngram_range=(1, noNGram))\n",
    "XTrainNGram = vectorizerNGram.fit_transform(trainSamples)\n",
    "\n",
    "print (vectorizerNGram)\n",
    "\n",
    "\n",
    "print (len(vectorizerNGram.get_feature_names()))\n",
    "trainNGramVector = XTrainNGram.toarray()\n",
    "print (trainNGramVector.shape)\n",
    "XTestNGram = vectorizerNGram.transform(testSamples)\n",
    "testNGramVector = XTestNGram.toarray()\n",
    "print (testNGramVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from helper import *\n",
    "# import utilities\n",
    "# from utilities import build_matrices\n",
    "\n",
    "# (matrix_train, matrix_eval, ngram_list) = build_matrices(max_ngram_length=3)\n",
    "# len(ngram_list)\n",
    "# matrix_train = sp.sparse.csr_matrix(matrix_train)\n",
    "# matrix_eval = sp.sparse.csr_matrix(matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noNGram = 3\n",
    "# vectorizerNGram = CountVectorizer(ngram_range=(1, noNGram))\n",
    "# XEvalNGram = vectorizerNGram.fit_transform(XEval)\n",
    "# print vectorizerNGram\n",
    "\n",
    "# print len(vectorizerNGram.get_feature_names())\n",
    "# evalNGramVector = XEvalNGram.toarray()\n",
    "# print evalNGramVector.shape\n",
    "\n",
    "# matrix_eval = sp.sparse.csr_matrix(evalNGramVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack or concatenate all features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 4909)\n",
      "(60,)\n",
      "(60, 4910)\n",
      "(19, 4910)\n",
      "(60, 4911)\n"
     ]
    }
   ],
   "source": [
    "XTrainWordFeatures = trainBagVector #trainNGramVector\n",
    "print (XTrainWordFeatures.shape)\n",
    "print (trainSentimentSamples.shape)\n",
    "\n",
    "temp = np.column_stack((XTrainWordFeatures, trainSentimentSamples))\n",
    "print (temp.shape)\n",
    "XTrainAllFeatures =  np.column_stack((temp, trainFreqTweetSamples))\n",
    "\n",
    "\n",
    "XTestWordFeatures = testBagVector #testNGramVector\n",
    "temp =  np.column_stack((XTestWordFeatures, testSentimentSamples))\n",
    "print (temp.shape)\n",
    "XTestAllFeatures =  np.column_stack((temp, testFreqTweetSamples))\n",
    "\n",
    "\n",
    "print (XTrainAllFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3995, 4916)\n"
     ]
    }
   ],
   "source": [
    "# XEvalWordFeatures = evalBagVector #evalNGramVector\n",
    "# temp = np.column_stack((XEvalWordFeatures, XEvalSentiment))\n",
    "XEvalAllFeatures =  np.column_stack((np.column_stack((evalBagVector, XEvalSentiment)), XEvalFreqTweets))\n",
    "\n",
    "print (XEvalAllFeatures.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predicted Output Labels to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writePredictedLabelFile(YPred):\n",
    "    f = open(\"Predictions.csv\",\"w\")\n",
    "    f.write(\"Id,Label\" + \"\\n\")\n",
    "    for i in range(len(YPred)):\n",
    "        f.write(str(i) + \",\" + str(int(YPred[i]))+ \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# def classifyRandomForestClassifier(XTrain, XTest, YTrain, YTest,trees=100,crit='gini'):\n",
    "def classifyRandomForestClassifier(XTrain, XTest, YTrain, YTest, params):\n",
    "    trees = params['trees']\n",
    "    crit = params['criterion']\n",
    "    seed = params['random_state']\n",
    "    clf = RandomForestClassifier(n_estimators=trees,criterion=crit,random_state=seed)\n",
    "    clf.fit(XTrain, YTrain)\n",
    "    YPred = clf.predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multi Class SVM\n",
    "from sklearn import svm\n",
    "def classifyMultiClassSVMClassifier(XTrain, XTest, YTrain, YTest, params):\n",
    "    ker = params['kernel']\n",
    "    YPred = svm.SVC(kernel=ker).fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K Nearest Neighbours Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def classifyKNNClassifier(XTrain, XTest, YTrain, YTest, params):\n",
    "#     print XTrain.shape, XTest.shape\n",
    "    neighbours = params['neighbours']\n",
    "    neigh = KNeighborsClassifier(n_neighbors=neighbours)\n",
    "    YPred = neigh.fit(XTrain, YTrain).predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn import linear_model\n",
    "def classifyLogisticRegression(XTrain, XTest, YTrain, YTest, params):\n",
    "    LogReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "    LogReg.fit(XTrain, YTrain)\n",
    "    # Finds the optimal model parameters using a least squares method.\n",
    "    # To get the parameter values:\n",
    "    # LogReg.get_params()\n",
    "    # To predict a new input XTest,\n",
    "    YPred = LogReg.predict(XTest)\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adaboost Classfier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def classifyAdaboostClassifier(XTrain, XTest, YTrain, YTest, params):\n",
    "    depth = params['max_depth']\n",
    "    algo = params['algorithm']\n",
    "    estimators = params['n_estimators']\n",
    "    \n",
    "    # Create and fit an AdaBoosted decision tree\n",
    "    bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth = depth),\n",
    "                         algorithm = algo,\n",
    "                         n_estimators=estimators)\n",
    "\n",
    "    bdt.fit(XTrain, YTrain)\n",
    "    YPred = bdt.predict(XTest)\n",
    "\n",
    "    diff = YPred - YTest\n",
    "    score = diff[diff == 0].size\n",
    "    return (100.0 * score)/(YPred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nTo use MKL 2018 with Theano either update the numpy conda packages to\ntheir latest build or set \"MKL_THREADING_LAYER=GNU\" in your\nenvironment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/configparser.py\u001b[0m in \u001b[0;36m_unify_values\u001b[0;34m(self, section, vars)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m             \u001b[0msectiondict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blas'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNoSectionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/configparser.py\u001b[0m in \u001b[0;36mfetch_val_for_key\u001b[0;34m(key, delete_key)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtheano_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpolationError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/configparser.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, section, option, raw, vars, fallback)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unify_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNoSectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/configparser.py\u001b[0m in \u001b[0;36m_unify_values\u001b[0;34m(self, section, vars)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msection\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_section\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNoSectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m         \u001b[0;31m# Update with the entry specific variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSectionError\u001b[0m: No section: 'blas'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/configparser.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, cls, type_, delete_key)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 val_str = fetch_val_for_key(self.fullname,\n\u001b[0;32m--> 328\u001b[0;31m                                             delete_key=delete_key)\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/configparser.py\u001b[0m in \u001b[0;36mfetch_val_for_key\u001b[0;34m(key, delete_key)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConfigParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoOptionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoSectionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blas.ldflags'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3319f51a5beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Neural Networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Please install scikit-neuralnetwork(pip install scikit-neuralnetwork)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m from theano.scan_module import (scan, map, reduce, foldl, foldr, clone,\n\u001b[0m\u001b[1;32m    125\u001b[0m                                 scan_checkpoints)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/scan_module/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0m__contact__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Razvan Pascanu <r.pascanu@gmail>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_checkpoints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/scan_module/scan_opt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scalar_constant_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAllocEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/tensor/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopt_uncanonicalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblas_scipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblas_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/tensor/blas.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbool_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblas_headers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblas_header_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblas_headers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblas_header_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0min2out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dimshuffle_lift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/tensor/blas_headers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldflags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using NumPy C-API based implementation for BLAS functions.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/configparser.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, cls, type_, delete_key)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                     \u001b[0mval_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mval_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/configdefaults.py\u001b[0m in \u001b[0;36mdefault_blas_ldflags\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_blas_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m                 \u001b[0mcheck_mkl_openmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m                 \u001b[0mmaybe_add_to_os_environ_pathlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PATH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/configdefaults.py\u001b[0m in \u001b[0;36mcheck_mkl_openmp\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mlatest\u001b[0m \u001b[0mbuild\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mset\u001b[0m \u001b[0;34m\"MKL_THREADING_LAYER=GNU\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m \"\"\")\n\u001b[0m\u001b[1;32m   1263\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         raise RuntimeError(\"\"\"\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nTo use MKL 2018 with Theano either update the numpy conda packages to\ntheir latest build or set \"MKL_THREADING_LAYER=GNU\" in your\nenvironment.\n"
     ]
    }
   ],
   "source": [
    "# Neural Networks\n",
    "try:\n",
    "    from sknn.mlp import Classifier, Layer\n",
    "except ImportError:\n",
    "    print ('Please install scikit-neuralnetwork(pip install scikit-neuralnetwork)')\n",
    "\n",
    "def classifyNeuralNetworkClassifier(XTrain, XTest, YTrain, YTest, params):\n",
    "    activation = params['activation']\n",
    "    actLastLayer = params['actLastLayer']\n",
    "    rule = params['rule']\n",
    "    noOfUnits = params['units']\n",
    "    rate = params['rate']\n",
    "    noOfIter = params['iter']\n",
    "    nn = Classifier(layers=[Layer(activation, units=noOfUnits),Layer(actLastLayer)], learning_rule=rule,\n",
    "        learning_rate=0.02,\n",
    "        n_iter=10)\n",
    "    nn.fit(XTrain, YTrain)\n",
    "    YPred = nn.predict(XTest)\n",
    "    diff = YPred - YTest.reshape(YPred.shape)\n",
    "    score = diff[diff == 0].size\n",
    "    score = (100.0 * score)/(YPred.size)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shail/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "def stratifiedKFoldVal(XTrain, YTrain, classify, params):\n",
    "    n_folds = 5\n",
    "    score = 0.0\n",
    "    skf = StratifiedKFold(YTrain, n_folds)\n",
    "    try:\n",
    "        multi = params['multi']\n",
    "    except KeyError:\n",
    "        multi = False\n",
    "    for train_index, test_index in skf:\n",
    "        y_train, y_test = YTrain[train_index], YTrain[test_index]\n",
    "        if not multi:\n",
    "            X_train, X_test = XTrain[train_index], XTrain[test_index]\n",
    "            score += classify(X_train, X_test,  y_train, y_test, params)\n",
    "        else:\n",
    "            X_train, X_test = [XTrain[i] for i in train_index], [XTrain[i] for i in test_index]\n",
    "            score += classify(np.array(X_train), np.array(X_test),  y_train, y_test, params)\n",
    "        \n",
    "    return score/n_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation of Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def NormalizeVector(XTestFeatures,XTrainFeatures):\n",
    "    XTestFeaturesNorm = preprocessing.normalize(XTestFeatures, norm='l2')\n",
    "    XTrainFeaturesNorm = preprocessing.normalize(XTrainFeatures, norm='l2')\n",
    "    print (XTrainFeaturesNorm.shape,XTestFeaturesNorm.shape)\n",
    "#     print XTrainFeaturesNorm[0],XTestFeaturesNorm[0]\n",
    "    return XTrainFeaturesNorm, XTestFeaturesNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Train features for cross validation based on the feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'> <type 'list'>\n",
      "(60, 4916)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "train = XTrainAllFeatures\n",
    "# train = tfidfTrain\n",
    "# train = trainStateTransitionVector\n",
    "print type(trainBagVector), type(trainStateTransitionVector)\n",
    "# train = []\n",
    "# for i in xrange(len(trainBagVector)):\n",
    "#     train.append(trainBagVector[i]+trainStateTransitionVector[i])\n",
    "# print len(train)\n",
    "# train = np.hstack([tfidfTrain, np.array(trainStateTransitionVector)])\n",
    "# train = np.hstack([trainBagVector, np.array(trainStateTransitionVector)])\n",
    "\n",
    "print train.shape\n",
    "YTrain = YtrainSamples\n",
    "print YTrain.shape\n",
    "YTest = YtestSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Nearest Neighbours for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selectNeighbourScores = []\n",
    "\n",
    "# params = {'neighbours':2}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'neighbours':3}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'neighbours':4}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'neighbours':5}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'neighbours':10}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'neighbours':25}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'neighbours':40}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# print selectNeighbourScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Plotting the results\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.plot(selectNeighbourScores, label = \"Neighbors in k-Nearest Neighbor (kNN) Classifier\")\n",
    "# plt.title(\"Neighbors in k-Nearest Neighbor (kNN) Classifier\")\n",
    "\n",
    "# labels = [2,3,4,5,6,8,10]\n",
    "# plt.xticks(np.arange(len(labels)), labels, rotation='horizontal')\n",
    "# # plt.title(\"Optimal choice of Neighbors in k-Nearest Neighbor (kNN) Classifier\")\n",
    "# plt.ylabel('Categorization Accuracy')\n",
    "# plt.xlabel('No. of Neighbours')\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, we choose k = 25 for our nearest neighbor classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 60\n"
     ]
    }
   ],
   "source": [
    "print len(testStateTransitionVector), len(trainStateTransitionVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = np.hstack([XTrainAllFeatures, XTestAllFeatures])\n",
    "train = XTrainAllFeatures\n",
    "test = XEvalAllFeatures\n",
    "params = {'neighbours':25}\n",
    "neighbours = params['neighbours']\n",
    "neigh = KNeighborsClassifier(n_neighbors=neighbours)\n",
    "YPred = neigh.fit(train, YTrain).predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 9 9 ..., 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "print YPred[2:3020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selectRandomForestScores = []\n",
    "\n",
    "# params = {'trees':500, 'criterion':'entropy','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# params = {'trees':1000, 'criterion':'entropy','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# params = {'trees':500, 'criterion':'gini','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# params = {'trees':1000, 'criterion':'gini','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# print selectRandomForestScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Plotting the results\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.plot(selectRandomForestScores, label = \"Random Forest Classifier\")\n",
    "# plt.title(\"Random Forest Classifier\")\n",
    "\n",
    "# labels = ['500 Trees + entropy', '1000 Trees + entropy', '500 Trees + gini', '1000 Trees + gini']\n",
    "\n",
    "# # You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "# plt.xticks(np.arange(len(labels)), labels, rotation='vertical')\n",
    "\n",
    "# plt.ylabel('Scores')\n",
    "# plt.xlabel('Parameters')\n",
    "# # Place a legend to the right of this smaller figure.\n",
    "# # plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, we choose 1000 Trees + Gini as a criterion for our Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'trees':150, 'criterion':'entropy','random_state':None}\n",
    "trees = params['trees']\n",
    "crit = params['criterion']\n",
    "seed = params['random_state']\n",
    "clf = RandomForestClassifier(n_estimators=trees,criterion=crit,random_state=seed)\n",
    "clf.fit(train, YTrain)\n",
    "YPred = clf.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Kernel for Multi Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selectKernelScores = []\n",
    "\n",
    "# params = {'kernel':'poly'}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyMultiClassSVMClassifier, params)\n",
    "# print score\n",
    "# selectKernelScores.append(score)\n",
    "\n",
    "# params = {'kernel':'linear'}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyMultiClassSVMClassifier, params)\n",
    "# print score\n",
    "# selectKernelScores.append(score)\n",
    "\n",
    "# params = {'kernel':'rbf'}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyMultiClassSVMClassifier, params)\n",
    "# print score\n",
    "# selectKernelScores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Plotting the results\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.plot(selectKernelScores, label = \"Multiclass SVM Classifier\")\n",
    "\n",
    "# labels = ['poly','linear','rbf']\n",
    "# plt.title(\"Multiclass SVM Classifier\")\n",
    "# # You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "# plt.xticks(np.arange(len(labels)), labels, rotation='horizontal')\n",
    "\n",
    "# plt.ylabel('Scores')\n",
    "# plt.xlabel('Kernel used')\n",
    "# # Place a legend to the right of this smaller figure.\n",
    "# # plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Hence, we choose rbf for our SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'kernel':'rbf'}\n",
    "ker = params['kernel']\n",
    "YPred = svm.SVC(kernel=ker).fit(train, YTrain).predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # params = {'multi':False}\n",
    "# # train = tfidfTrain\n",
    "# # score = stratifiedKFoldVal(train, YTrain, classifyLogisticRegression, params)\n",
    "# # print score\n",
    "# train = XTrainAllFeatures\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyLogisticRegression, params)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params = {'multi':True}\n",
    "# train = trainStateTransitionVector\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyLogisticRegression, params)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LogReg = linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None)\n",
    "# LogReg.fit(trainBagVector, YTrain)\n",
    "# YPred = LogReg.predict(testBagVector)\n",
    "# # writePredictedLabelFile(YPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters for Adaboost and use it on different training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = XTrainAllFeatures\n",
    "# params = {'max_depth':1, 'algorithm':'SAMME', 'n_estimators':200}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyAdaboostClassifier, params)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = XTrainAllFeatures\n",
    "# params = {'max_depth':10, 'algorithm':'SAMME', 'n_estimators':500}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyAdaboostClassifier, params)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Submission\n",
    "# params = {'max_depth':10, 'algorithm':'SAMME', 'n_estimators':500}\n",
    "# train = tfidfTrain\n",
    "# test = tfidfTest\n",
    "# depth = params['max_depth']\n",
    "# algo = params['algorithm']\n",
    "# estimators = params['n_estimators']\n",
    "\n",
    "# # Create and fit an AdaBoosted decision tree\n",
    "# bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth = depth),\n",
    "#                      algorithm = algo,\n",
    "#                      n_estimators=estimators)\n",
    "\n",
    "# bdt.fit(train, YTrain)\n",
    "# YPred = bdt.predict(test)\n",
    "# # writePredictedLabelFile(YPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Parameters for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = XTrainAllFeatures\n",
    "# # params = {'activation':'Rectifier', 'units':100, 'rate':0.02, 'iter':10}\n",
    "# params = {'activation':'Tanh', 'actLastLayer':'Softmax', 'rule':'momentum', 'units':100, 'rate':0.002, 'iter':10}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyNeuralNetworkClassifier, params)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = XTrainAllFeatures\n",
    "# # params = {'activation':'Rectifier', 'units':100, 'rate':0.02, 'iter':10}\n",
    "# params = {'activation':'Tanh', 'actLastLayer':'Softmax', 'rule':'sgd', 'units':100, 'rate':0.002, 'iter':10}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyNeuralNetworkClassifier, params)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = XTrainAllFeatures\n",
    "# params = {'activation':'Sigmoid', 'actLastLayer':'Softmax', 'rule':'rmsprop', 'units':100, 'rate':0.002, 'iter':10}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyNeuralNetworkClassifier, params)\n",
    "# print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Submission\n",
    "# tr = trainBagVector\n",
    "# te = testBagVector\n",
    "# params = {'activation':'Tanh', 'actLastLayer':'Softmax', 'rule':'adagrad', 'units':100, 'rate':0.002, 'iter':10}\n",
    "# activation = params['activation']\n",
    "# actLastLayer = params['actLastLayer']\n",
    "# rule = params['rule']\n",
    "# noOfUnits = params['units']\n",
    "# rate = params['rate']\n",
    "# noOfIter = params['iter']\n",
    "# nn = Classifier(layers=[Layer(activation, units=noOfUnits),Layer(actLastLayer)], learning_rule=rule,\n",
    "#         learning_rate=0.02,\n",
    "#         n_iter=10)\n",
    "# nn.fit(tr, YTrain)\n",
    "# YPred = nn.predict(te)\n",
    "# # writePredictedLabelFile(YPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get features in format for Models of NLTK Classify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featNLTKClassify(samples, phase):\n",
    "    featureVectors = vectorizer.get_feature_names()\n",
    "    nltkClassifySamples = []\n",
    "\n",
    "    for i in xrange(len(samples)):\n",
    "        t = samples[i]\n",
    "        lstFuncCalls = t.split()\n",
    "        wordOccDict = {}\n",
    "        for j in xrange(len(featureVectors)):\n",
    "             wordOccDict[featureVectors[j]] = lstFuncCalls.count(featureVectors[j])\n",
    "        if phase == 'train':\n",
    "            nltkClassifySamples.append((wordOccDict, YTrain[i]))\n",
    "        else:\n",
    "            nltkClassifySamples.append(wordOccDict)\n",
    "\n",
    "    return nltkClassifySamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltkClassifyTrain = featNLTKClassify(trainSamples, 'train')\n",
    "# nltkClassifyTest = featNLTKClassify(testSamples, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nave Baiyes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tr = nltkClassifyTrain\n",
    "# te = nltkClassifyTest\n",
    "# classifier = nltk.classify.NaiveBayesClassifier.train(tr)\n",
    "# sorted(classifier.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier.classify_many(te)\n",
    "\n",
    "# classifier.show_most_informative_features()\n",
    "# # print nltk.classify.accuracy(classifier, te)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.classify import maxent\n",
    "# tr = nltkClassifyTrain\n",
    "# te = nltkClassifyTest\n",
    "# classifierME = maxent.MaxentClassifier.train(tr, bernoulli=False, encoding=encoding, trace=0)\n",
    "# classifierME.classify_many(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tr = nltkClassifyTrain\n",
    "# te = nltkClassifyTest\n",
    "\n",
    "# classifier = nltk.classify.DecisionTreeClassifier.train(tr, entropy_cutoff=0,support_cutoff=0)\n",
    "# sorted(classifier.labels())\n",
    "# print(classifier)\n",
    "# classifier.classify_many(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs depicting Categorization Accuracy scores on KFold Stratified Validation on Train data for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selectRandomForestScores = []\n",
    "# selectKernelScores = []\n",
    "# selectNeighbourScores = []\n",
    "\n",
    "# train = trainBagVector\n",
    "\n",
    "# params = {'trees':1000, 'criterion':'gini','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# params = {'neighbours':25}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'kernel':'linear'}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyMultiClassSVMClassifier, params)\n",
    "# print score\n",
    "# selectKernelScores.append(score)\n",
    "\n",
    "\n",
    "\n",
    "# train = tfidfTrain\n",
    "\n",
    "# params = {'trees':1000, 'criterion':'gini','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# params = {'neighbours':25}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'kernel':'rbf'}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyMultiClassSVMClassifier, params)\n",
    "# print score\n",
    "# selectKernelScores.append(score)\n",
    "\n",
    "\n",
    "# train = np.array(trainStateTransitionVector)\n",
    "\n",
    "# params = {'trees':1000, 'criterion':'gini','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# params = {'neighbours':25}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "\n",
    "# params = {'kernel':'rbf'}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyMultiClassSVMClassifier, params)\n",
    "# print score\n",
    "# selectKernelScores.append(score)\n",
    "\n",
    "\n",
    "# train = np.hstack([trainBagVector, np.array(trainStateTransitionVector)])\n",
    "\n",
    "# params = {'trees':1000, 'criterion':'gini','random_state':1000}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyRandomForestClassifier, params)\n",
    "# print score\n",
    "# selectRandomForestScores.append(score)\n",
    "\n",
    "# params = {'neighbours':25}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyKNNClassifier, params)\n",
    "# print score\n",
    "# selectNeighbourScores.append(score)\n",
    "\n",
    "# params = {'kernel':'rbf'}\n",
    "# score = stratifiedKFoldVal(train, YTrain, classifyMultiClassSVMClassifier, params)\n",
    "# print score\n",
    "# selectKernelScores.append(score)\n",
    "\n",
    "\n",
    "\n",
    "# print selectRandomForestScores\n",
    "# print selectKernelScores\n",
    "# print selectNeighbourScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Plotting the results\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.plot(selectRandomForestScores, label = \"Random Forest Classifier\")\n",
    "# plt.plot(selectKernelScores, label = \"Multiclass Linear SVM Classifier\")\n",
    "# plt.plot(selectNeighbourScores, label = \"KNN Classifier\")\n",
    "\n",
    "# labels = ['Bag of Words', 'TF-IDF', 'State Transitions', 'Stacked Features 1 & 3']\n",
    "\n",
    "# # You can specify a rotation for the tick labels in degrees or with keywords.\n",
    "# plt.xticks(np.arange(len(labels)), labels, rotation='vertical')\n",
    "\n",
    "# plt.ylabel('Scores')\n",
    "# plt.xlabel('Feature Encoding used')\n",
    "# # Place a legend to the right of this smaller figure.\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar graph depicting Categorization Accuracy Scores on the different Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# N = 8\n",
    "# publicScore = (80.453, 75.637, 79.887, 80.737, 81.586, 80.170, 80.170, 79.887)\n",
    "# privateScore = (84.136, 80.170, 83.569, 83.003, 83.286, 83.003, 83.003, 83.569)\n",
    "# modelNames = ('RF(50T, Entropy)+Bag of Words', 'RF(150T, Entropy)+TF-IDF', 'RF(50T, Entropy) + State Transition', \n",
    "#               'KNN(5) +Bag of Words', 'KNN(5) + TF-IDF', 'KNN(5) + State Transition',\n",
    "#               'Stack: KNN(5) + ST + BoW', 'Stack: RF(50T, Entropy) + ST + BoW')\n",
    "\n",
    "# ind = np.arange(N)  # the x locations for the groups\n",
    "# width = 0.35       # the width of the bars\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# rects1 = ax.bar(ind, publicScore, width, color='m')\n",
    "\n",
    "# rects2 = ax.bar(ind + width, privateScore, width, color='c')\n",
    "\n",
    "# # add some text for labels, title and axes ticks\n",
    "# ax.set_ylabel('Scores')\n",
    "# ax.set_title('Evaluations of submissions using Categorization Accuracy.')\n",
    "# ax.set_xticks(ind + width)\n",
    "# ax.set_xticklabels(modelNames,  rotation='vertical')\n",
    "# ax.set_ylim(75,85)\n",
    "\n",
    "# # def autolabel(rects):\n",
    "# #     # attach some text labels\n",
    "# #     for rect in rects:\n",
    "# #         height = rect.get_height()\n",
    "# #         ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "# #                 '%d' % int(height),\n",
    "# #                 ha='center', va='bottom')\n",
    "\n",
    "# # autolabel(rects1)\n",
    "# # autolabel(rects2)\n",
    "\n",
    "# # Place a legend to the right of this smaller figure.\n",
    "# ax.legend((rects1[0], rects2[0]), ('Public Scores', 'Private Scores'), bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence we conclude that the best model is kNN using TF-IDF as features !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverseMapLabels(classNo):\n",
    "    if className == 0:\n",
    "        return 'Conscientiousness'\n",
    "    elif className == 1:\n",
    "        return 'Extrovert'\n",
    "    elif className == 2:\n",
    "        return 'Agreeable'\n",
    "    elif className == 3:\n",
    "        return 'Empathetic'\n",
    "    elif className == 4:\n",
    "        return 'Novelty Seeking'\n",
    "    elif className == 5:\n",
    "        return 'Perfectionist'\n",
    "    elif className == 6:\n",
    "        return 'Rigid'\n",
    "    elif className == 7:\n",
    "        return 'Impulsive'\n",
    "    elif className == 8:\n",
    "        return 'Psychopath'\n",
    "    elif className == 9:\n",
    "        return 'Obsessive'\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def GeoPlot(geo_longitude, geo_latitude, labels):\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    raw_data = {'latitude': geo_latitude,'longitude': geo_longitude}\n",
    "\n",
    "    df = pd.DataFrame(raw_data, columns = ['latitude', 'longitude'])\n",
    "    \n",
    "    totSampleLen = len(labels)\n",
    "#     print totSampleLen\n",
    "    colors = ['blue', 'beige', 'red', 'green', 'magenta', 'yellow', 'cyan', 'aquamarine', 'azure', 'darkkhaki']\n",
    "    \n",
    "    m = Basemap(projection='gall',lon_0=0,lat_0=0,resolution='i')\n",
    "#     x1,y1=map(geo_longitude, geo_latitude)\n",
    "    x1,y1 = map(df['longitude'].values, df['latitude'].values)\n",
    "\n",
    "\n",
    "    m.drawmapboundary(fill_color='black') # fill to edge\n",
    "    m.drawcountries()\n",
    "    m.fillcontinents(color='white',lake_color='black')\n",
    "    \n",
    "#     m.scatter(x1, y1, marker='D',color='m', s=2)\n",
    "    for i in xrange(totSampleLen):\n",
    "        for k in xrange(10):\n",
    "            if labels[i] == k:\n",
    "#                 print x1[i], y1[i]\n",
    "#                 print colors[k]\n",
    "#                 m.scatter(x1[i], y1[i], marker='D',color=colors[k], s=2)\n",
    "                m.plot(x1[i], y1[i], 'ro', color=colors[k]) #'ro', markersize=6)\n",
    "\n",
    "    \n",
    "    for k in xrange(10):\n",
    "        m.scatter(0,0, marker='D',color=colors[k], s=2, label=reverseMapLabels(k))\n",
    "    \n",
    "    plt.title(\"Geo-tagging Personality Types for Twitter Users\")\n",
    "    # Place a legend to the right of this smaller figure.\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Personality Types based on location of user tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(60,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-1568fa9e8ce9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mGeoPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeo_longitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeo_latitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-84-9820463ead0e>\u001b[0m in \u001b[0;36mGeoPlot\u001b[1;34m(geo_longitude, geo_latitude, labels)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gall'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlon_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlat_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresolution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#     x1,y1=map(geo_longitude, geo_latitude)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "lon = np.random.random_integers(-180,180,60)\n",
    "lat = np.random.random_integers(-90,90,60)\n",
    "geo_latitude = lat\n",
    "geo_longitude = lon\n",
    "print type(lat)\n",
    "print lat.shape\n",
    "GeoPlot(geo_longitude, geo_latitude, YTrain[0:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GeoPlot(eval_geo_longitude[0:1000], eval_geo_latitude[0:1000], YPred[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-tagging Sentiments of Twitter Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverseMapSentiments(classNo):\n",
    "    if classNo == 0:\n",
    "        return 'Negative'\n",
    "    elif classNo == 1:\n",
    "        return 'Neutral'\n",
    "    elif classNo == 2:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GeoSentimentPlot(geo_longitude, geo_latitude, sentiments):\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    raw_data = {'latitude': geo_latitude,\n",
    "            'longitude': geo_longitude}\n",
    "\n",
    "    df = pd.DataFrame(raw_data, columns = ['latitude', 'longitude'])\n",
    "\n",
    "    \n",
    "    totSampleLen = len(sentiments)\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    \n",
    "    negLimit = 0\n",
    "    posLimit = 0\n",
    "    \n",
    "    m = Basemap(projection='gall',lon_0=0,lat_0=0,resolution='i')\n",
    "    \n",
    "    x1,y1 = map(df['longitude'].values, df['latitude'].values)\n",
    "\n",
    "    m.drawmapboundary(fill_color='black')\n",
    "    m.drawcountries()\n",
    "    m.fillcontinents(color='white',lake_color='black')\n",
    "    \n",
    "    for i in xrange(totSampleLen):\n",
    "#         print sentiments[i]\n",
    "        if sentiments[i] < negLimit:\n",
    "            m.plot(x1[i], y1[i], 'ro', color=colors[0])\n",
    "        elif sentiments[i] >= negLimit and sentiments[i] <= posLimit:\n",
    "            m.plot(x1[i], y1[i], 'ro', color=colors[1])\n",
    "        elif sentiments[i] > posLimit:\n",
    "            m.plot(x1[i], y1[i], 'ro', color=colors[2])\n",
    "    \n",
    "    \n",
    "    for k in xrange(3):\n",
    "        m.scatter(0,0, marker='D',color=colors[k], s=2, label=reverseMapSentiments(k))\n",
    "    \n",
    "    plt.title(\"Geo-tagging Sentiments of Twitter Users\")\n",
    "    # Place a legend to the right of this smaller figure.\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sentiment of user tweets based on location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-5493af4d1802>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGeoSentimentPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeo_longitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeo_latitude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXTrainSentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-87-58f1c2b2cf97>\u001b[0m in \u001b[0;36mGeoSentimentPlot\u001b[1;34m(geo_longitude, geo_latitude, sentiments)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gall'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlon_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlat_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresolution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawmapboundary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "GeoSentimentPlot(geo_longitude, geo_latitude, XTrainSentiment[0:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(eval_geo_longitude)\n",
    "eval_geo_longitude = np.array(eval_geo_longitude)\n",
    "eval_geo_latitude = np.array(eval_geo_latitude)\n",
    "print len(eval_geo_longitude)\n",
    "print eval_geo_longitude.shape\n",
    "print type(eval_geo_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GeoSentimentPlot(eval_geo_longitude[0:1000], eval_geo_latitude[0:1000], XEvalSentiment[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
